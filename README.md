# Pipeline de ETL simples

 Como parte da minha jornada de aprendizado em Engenharia de Dados, enfrentei o desafio de desenvolver um processo de ETL (Extra√ß√£o, Transforma√ß√£o e Carga) simples, mas que envolvia diversos conceitos de base. Neste Readme, vou compartilhar um pouco sobre essa experi√™ncia e as li√ß√µes que aprendi.


## <br/>üåü **Contexto do Neg√≥cio**
O projeto envolveu dados de vendas di√°rias fornecidos por uma API de um e-commerce (nesse caso de estudo, eu j√° estava com os .json tratados e dispon√≠veis). 


## <br/>üéØ **Qual o desafio?**

Criar uma ETL simples para ler arquivos do tipo Json, concatenar diversos esses arquivos, fazer algumas transforma√ß√£o nos dados e ao final, poder ter escolher entre duas sa√≠das, um .csv ou um .parquet



## <br/>üõ§Ô∏è Passos Cruciais do Projeto

### 1. Desenhar o Fluxo da Solu√ß√£o:

<img src="image-1.png"
width="100px"/>


**<br/>As tarefas s√£o as seguintes:**
1. Ler arquivos JSON.
2. Concatenar v√°rios arquivos JSON.
3. Transformar os dados usando a biblioteca Pandas.
4. Exportar os dados transformados em CSV ou Parquet.



**<br/>Com o fluxo acima, √© poss√≠vel:**

-  Entender o fluxo de dados desde a extra√ß√£o, transforma√ß√£o at√© a carga dos dados.
- Mapeamento dos pontos de integra√ß√£o e transforma√ß√µes necess√°rias.



### <br/>2. Leitura de Arquivos JSON:

- Analisar a melhor abordagem para leitura dos arquivos
- Decidir entre utilizar Python puro ou bibliotecas especializadas.
    - Nesse caso, optei por utilizar a biblioteca pandas pois j√° √©  amplamenta aceita e utiliza comercialmente.


### <br/>3. Transforma√ß√µes nos dados:

- Realizar as transforma√ß√µes necess√°rias nos dados, como limpeza, agrega√ß√£o e convers√µes.
- Garantir a qualidade dos dados utilizando ferramentas como Pydantic e Pandera para valida√ß√µes e consist√™ncia.


## <br/>üìö **Conceitos Fundamentais Aprendidos**
**ETL:** Compreens√£o profunda de cada fase do processo ETL e sua import√¢ncia na integra√ß√£o de dados.
**<br/><br/>Qualidade de Dados:** Valida√ß√£o rigorosa da integridade dos dados ao longo do pipeline.
**<br/><br/>Ferramentas:** Uso estrat√©gico de Pandas, Pydantic e Pandera, entendendo a escolha da ferramenta certa para cada etapa.


## <br/>**üöÄ Pr√≥ximos Passos**
**Aprimoramento:** Otimizar o pipeline com novas ferramentas e t√©cnicas para melhorar a escalabilidade e o desempenho.
**<br/><br/>Explora√ß√£o:** Aplicar os conhecimentos adquiridos em projetos mais complexos, continuando a expandir meu dom√≠nio em Engenharia de Dados.